"""
Economic Data Augmentation
Advanced data augmentation techniques for economic time series to improve model training and handle data scarcity.
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Union, Tuple, Callable
import logging
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

from scipy import stats, signal
from scipy.interpolate import interp1d
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors

logger = logging.getLogger(__name__)


class EconomicDataAugmentor:
    """Advanced data augmentation for economic time series."""
    
    def __init__(self, random_seed: int = 42):
        """
        Initialize data augmentor.
        
        Args:
            random_seed: Random seed for reproducibility
        """
        self.random_seed = random_seed
        np.random.seed(random_seed)
        
        # Augmentation techniques
        self.techniques = {
            'noise_injection': self.noise_injection,
            'time_warping': self.time_warping,
            'magnitude_scaling': self.magnitude_scaling,
            'window_slicing': self.window_slicing,
            'frequency_masking': self.frequency_masking,
            'mixup': self.mixup,
            'cutmix': self.cutmix,
            'seasonal_adjustment': self.seasonal_adjustment,
            'trend_modification': self.trend_modification,
            'outlier_injection': self.outlier_injection,
            'missing_data_simulation': self.missing_data_simulation,
            'regime_switching': self.regime_switching
        }
        
        # Economic domain knowledge
        self.economic_constraints = {
            'gdp_growth': {'min': -0.15, 'max': 0.12, 'volatility_scale': 0.5},
            'inflation': {'min': -0.05, 'max': 0.20, 'volatility_scale': 0.3},
            'unemployment': {'min': 0.01, 'max': 0.25, 'volatility_scale': 0.4},
            'interest_rate': {'min': 0.0, 'max': 0.20, 'volatility_scale': 0.6},
            'stock_returns': {'min': -0.60, 'max': 0.60, 'volatility_scale': 1.0}
        }
        
        logger.info("Economic data augmentor initialized")
    
    def augment_dataset(self, 
                       data: Union[pd.Series, pd.DataFrame],
                       techniques: List[str] = None,
                       augmentation_factor: int = 5,
                       preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:
        """
        Augment economic dataset using multiple techniques.
        
        Args:
            data: Original economic data
            techniques: List of augmentation techniques to use
            augmentation_factor: Number of augmented samples per original
            preserve_properties: Whether to preserve statistical properties
        
        Returns:
            Augmented dataset
        """
        if techniques is None:
            techniques = ['noise_injection', 'time_warping', 'magnitude_scaling', 
                         'window_slicing', 'seasonal_adjustment']
        
        logger.info(f"Augmenting dataset with {len(techniques)} techniques, factor: {augmentation_factor}")
        
        augmented_data = []
        
        # Keep original data
        augmented_data.append(data)
        
        # Apply each technique multiple times
        for _ in range(augmentation_factor):
            for technique in techniques:
                if technique in self.techniques:
                    try:
                        augmented_sample = self.techniques[technique](
                            data, preserve_properties=preserve_properties
                        )
                        augmented_data.append(augmented_sample)
                    except Exception as e:
                        logger.warning(f"Failed to apply {technique}: {e}")
                else:
                    logger.warning(f"Unknown technique: {technique}")
        
        # Combine augmented data
        if isinstance(data, pd.Series):
            # For series, create a list of series
            logger.info(f"Generated {len(augmented_data)} augmented series")
            return augmented_data
        else:
            # For DataFrame, concatenate
            combined_data = pd.concat(augmented_data, ignore_index=True)
            logger.info(f"Augmented dataset shape: {combined_data.shape}")
            return combined_data
    
    def noise_injection(self, 
                       data: Union[pd.Series, pd.DataFrame],
                       noise_level: float = 0.05,
                       noise_type: str = 'gaussian',
                       preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:
        """
        Add controlled noise to economic data.
        
        Args:
            data: Original data
            noise_level: Noise level as fraction of data standard deviation
            noise_type: Type of noise ('gaussian', 'uniform', 'laplace')
            preserve_properties: Whether to preserve statistical properties
        
        Returns:
            Data with added noise
        """
        data_copy = data.copy()
        
        if isinstance(data, pd.Series):
            data_std = data.std()
            noise_std = noise_level * data_std
            
            if noise_type == 'gaussian':
                noise = np.random.normal(0, noise_std, len(data))
            elif noise_type == 'uniform':
                noise = np.random.uniform(-noise_std*1.73, noise_std*1.73, len(data))
            elif noise_type == 'laplace':
                noise = np.random.laplace(0, noise_std/1.41, len(data))
            else:
                raise ValueError(f"Unknown noise type: {noise_type}")
            
            data_copy += noise
            
            # Apply economic constraints if available
            if data.name in self.economic_constraints:
                constraints = self.economic_constraints[data.name]
                data_copy = np.clip(data_copy, constraints['min'], constraints['max'])
        
        else:  # DataFrame
            for col in data.columns:
                col_std = data[col].std()
                noise_std = noise_level * col_std
                
                if noise_type == 'gaussian':\n                    noise = np.random.normal(0, noise_std, len(data))\n                elif noise_type == 'uniform':\n                    noise = np.random.uniform(-noise_std*1.73, noise_std*1.73, len(data))\n                elif noise_type == 'laplace':\n                    noise = np.random.laplace(0, noise_std/1.41, len(data))\n                else:\n                    noise = np.random.normal(0, noise_std, len(data))\n                \n                data_copy[col] += noise\n                \n                # Apply constraints\n                if col in self.economic_constraints:\n                    constraints = self.economic_constraints[col]\n                    data_copy[col] = np.clip(data_copy[col], constraints['min'], constraints['max'])\n        \n        return data_copy\n    \n    def time_warping(self, \n                    data: Union[pd.Series, pd.DataFrame],\n                    warp_factor: float = 0.2,\n                    preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Apply time warping to economic data.\"\"\"\n        if isinstance(data, pd.Series):\n            return self._time_warp_series(data, warp_factor)\n        else:\n            warped_data = data.copy()\n            for col in data.columns:\n                warped_data[col] = self._time_warp_series(data[col], warp_factor)\n            return warped_data\n    \n    def _time_warp_series(self, series: pd.Series, warp_factor: float) -> pd.Series:\n        \"\"\"Apply time warping to a single series.\"\"\"\n        n = len(series)\n        \n        # Create warping function\n        warp_points = int(n * 0.1)  # 10% of points as control points\n        control_indices = np.linspace(0, n-1, warp_points)\n        \n        # Add random warping\n        warp_offsets = np.random.normal(0, warp_factor * n / 10, warp_points)\n        warped_indices = control_indices + warp_offsets\n        warped_indices = np.clip(warped_indices, 0, n-1)\n        \n        # Interpolate to get full warping function\n        interp_func = interp1d(control_indices, warped_indices, \n                              kind='cubic', bounds_error=False, fill_value='extrapolate')\n        full_warp = interp_func(np.arange(n))\n        full_warp = np.clip(full_warp, 0, n-1)\n        \n        # Apply warping\n        original_indices = np.arange(n)\n        warped_values = np.interp(original_indices, full_warp, series.values)\n        \n        return pd.Series(warped_values, index=series.index, name=series.name)\n    \n    def magnitude_scaling(self, \n                         data: Union[pd.Series, pd.DataFrame],\n                         scale_range: Tuple[float, float] = (0.8, 1.2),\n                         preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Apply magnitude scaling to economic data.\"\"\"\n        scale_factor = np.random.uniform(scale_range[0], scale_range[1])\n        \n        if isinstance(data, pd.Series):\n            scaled_data = data * scale_factor\n            \n            # Apply constraints\n            if data.name in self.economic_constraints:\n                constraints = self.economic_constraints[data.name]\n                scaled_data = np.clip(scaled_data, constraints['min'], constraints['max'])\n            \n            return scaled_data\n        else:\n            scaled_data = data.copy()\n            for col in data.columns:\n                col_scale = np.random.uniform(scale_range[0], scale_range[1])\n                scaled_data[col] = data[col] * col_scale\n                \n                # Apply constraints\n                if col in self.economic_constraints:\n                    constraints = self.economic_constraints[col]\n                    scaled_data[col] = np.clip(scaled_data[col], constraints['min'], constraints['max'])\n            \n            return scaled_data\n    \n    def window_slicing(self, \n                      data: Union[pd.Series, pd.DataFrame],\n                      window_size: Optional[int] = None,\n                      preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Extract random windows from economic data.\"\"\"\n        n = len(data)\n        \n        if window_size is None:\n            window_size = int(n * np.random.uniform(0.7, 0.95))  # 70-95% of original\n        \n        # Random start position\n        max_start = n - window_size\n        start_idx = np.random.randint(0, max_start + 1)\n        end_idx = start_idx + window_size\n        \n        return data.iloc[start_idx:end_idx].copy()\n    \n    def frequency_masking(self, \n                         data: Union[pd.Series, pd.DataFrame],\n                         mask_fraction: float = 0.1,\n                         preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Apply frequency domain masking to economic data.\"\"\"\n        if isinstance(data, pd.Series):\n            return self._frequency_mask_series(data, mask_fraction)\n        else:\n            masked_data = data.copy()\n            for col in data.columns:\n                masked_data[col] = self._frequency_mask_series(data[col], mask_fraction)\n            return masked_data\n    \n    def _frequency_mask_series(self, series: pd.Series, mask_fraction: float) -> pd.Series:\n        \"\"\"Apply frequency masking to a single series.\"\"\"\n        # FFT\n        fft_data = np.fft.fft(series.values)\n        freqs = np.fft.fftfreq(len(series))\n        \n        # Randomly mask some frequencies\n        n_mask = int(len(fft_data) * mask_fraction)\n        mask_indices = np.random.choice(len(fft_data), n_mask, replace=False)\n        \n        # Apply mask (set to zero)\n        fft_masked = fft_data.copy()\n        fft_masked[mask_indices] = 0\n        \n        # Inverse FFT\n        masked_data = np.fft.ifft(fft_masked).real\n        \n        return pd.Series(masked_data, index=series.index, name=series.name)\n    \n    def mixup(self, \n             data1: Union[pd.Series, pd.DataFrame],\n             data2: Union[pd.Series, pd.DataFrame],\n             alpha: float = 0.4,\n             preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Apply mixup augmentation between two samples.\"\"\"\n        # Sample mixing coefficient\n        lam = np.random.beta(alpha, alpha)\n        \n        # Ensure same length\n        min_len = min(len(data1), len(data2))\n        data1_truncated = data1.iloc[:min_len]\n        data2_truncated = data2.iloc[:min_len]\n        \n        # Mix the data\n        mixed_data = lam * data1_truncated + (1 - lam) * data2_truncated\n        \n        if isinstance(data1, pd.Series):\n            # Apply constraints\n            if data1.name in self.economic_constraints:\n                constraints = self.economic_constraints[data1.name]\n                mixed_data = np.clip(mixed_data, constraints['min'], constraints['max'])\n        else:\n            for col in mixed_data.columns:\n                if col in self.economic_constraints:\n                    constraints = self.economic_constraints[col]\n                    mixed_data[col] = np.clip(mixed_data[col], constraints['min'], constraints['max'])\n        \n        return mixed_data\n    \n    def cutmix(self, \n              data1: Union[pd.Series, pd.DataFrame],\n              data2: Union[pd.Series, pd.DataFrame],\n              cut_ratio: float = 0.3,\n              preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Apply cutmix augmentation between two samples.\"\"\"\n        # Ensure same length\n        min_len = min(len(data1), len(data2))\n        data1_truncated = data1.iloc[:min_len].copy()\n        data2_truncated = data2.iloc[:min_len].copy()\n        \n        # Determine cut region\n        cut_size = int(min_len * cut_ratio)\n        cut_start = np.random.randint(0, min_len - cut_size + 1)\n        cut_end = cut_start + cut_size\n        \n        # Apply cut and mix\n        if isinstance(data1, pd.Series):\n            mixed_data = data1_truncated.copy()\n            mixed_data.iloc[cut_start:cut_end] = data2_truncated.iloc[cut_start:cut_end]\n        else:\n            mixed_data = data1_truncated.copy()\n            mixed_data.iloc[cut_start:cut_end] = data2_truncated.iloc[cut_start:cut_end]\n        \n        return mixed_data\n    \n    def seasonal_adjustment(self, \n                           data: Union[pd.Series, pd.DataFrame],\n                           seasonal_strength: float = None,\n                           preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Adjust seasonal patterns in economic data.\"\"\"\n        if isinstance(data, pd.Series):\n            return self._adjust_seasonality_series(data, seasonal_strength)\n        else:\n            adjusted_data = data.copy()\n            for col in data.columns:\n                adjusted_data[col] = self._adjust_seasonality_series(data[col], seasonal_strength)\n            return adjusted_data\n    \n    def _adjust_seasonality_series(self, series: pd.Series, seasonal_strength: float = None) -> pd.Series:\n        \"\"\"Adjust seasonality in a single series.\"\"\"\n        n = len(series)\n        \n        if seasonal_strength is None:\n            seasonal_strength = np.random.uniform(0.5, 2.0)\n        \n        # Add or modify seasonal component\n        seasonal_period = 12  # Assume monthly data\n        t = np.arange(n)\n        \n        # Multiple seasonal components\n        seasonal_component = (\n            seasonal_strength * 0.01 * np.sin(2 * np.pi * t / seasonal_period) +\n            seasonal_strength * 0.005 * np.sin(4 * np.pi * t / seasonal_period) +\n            seasonal_strength * 0.003 * np.sin(6 * np.pi * t / seasonal_period)\n        )\n        \n        adjusted_data = series + seasonal_component\n        \n        # Apply constraints\n        if series.name in self.economic_constraints:\n            constraints = self.economic_constraints[series.name]\n            adjusted_data = np.clip(adjusted_data, constraints['min'], constraints['max'])\n        \n        return pd.Series(adjusted_data, index=series.index, name=series.name)\n    \n    def trend_modification(self, \n                          data: Union[pd.Series, pd.DataFrame],\n                          trend_strength: float = None,\n                          preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Modify trend patterns in economic data.\"\"\"\n        if isinstance(data, pd.Series):\n            return self._modify_trend_series(data, trend_strength)\n        else:\n            modified_data = data.copy()\n            for col in data.columns:\n                modified_data[col] = self._modify_trend_series(data[col], trend_strength)\n            return modified_data\n    \n    def _modify_trend_series(self, series: pd.Series, trend_strength: float = None) -> pd.Series:\n        \"\"\"Modify trend in a single series.\"\"\"\n        n = len(series)\n        \n        if trend_strength is None:\n            trend_strength = np.random.uniform(-0.002, 0.002)  # Small trend change\n        \n        # Linear trend modification\n        t = np.arange(n)\n        trend_component = trend_strength * t\n        \n        modified_data = series + trend_component\n        \n        # Apply constraints\n        if series.name in self.economic_constraints:\n            constraints = self.economic_constraints[series.name]\n            modified_data = np.clip(modified_data, constraints['min'], constraints['max'])\n        \n        return pd.Series(modified_data, index=series.index, name=series.name)\n    \n    def outlier_injection(self, \n                         data: Union[pd.Series, pd.DataFrame],\n                         outlier_probability: float = 0.05,\n                         outlier_magnitude: float = 3.0,\n                         preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Inject outliers into economic data to simulate rare events.\"\"\"\n        if isinstance(data, pd.Series):\n            return self._inject_outliers_series(data, outlier_probability, outlier_magnitude)\n        else:\n            outlier_data = data.copy()\n            for col in data.columns:\n                outlier_data[col] = self._inject_outliers_series(\n                    data[col], outlier_probability, outlier_magnitude\n                )\n            return outlier_data\n    \n    def _inject_outliers_series(self, series: pd.Series, \n                               outlier_probability: float, \n                               outlier_magnitude: float) -> pd.Series:\n        \"\"\"Inject outliers into a single series.\"\"\"\n        data_copy = series.copy()\n        n = len(series)\n        data_std = series.std()\n        \n        # Determine outlier positions\n        outlier_mask = np.random.random(n) < outlier_probability\n        \n        # Generate outliers\n        outlier_values = np.random.choice([-1, 1], size=n) * outlier_magnitude * data_std\n        \n        # Apply outliers\n        data_copy[outlier_mask] += outlier_values[outlier_mask]\n        \n        # Apply constraints\n        if series.name in self.economic_constraints:\n            constraints = self.economic_constraints[series.name]\n            data_copy = np.clip(data_copy, constraints['min'], constraints['max'])\n        \n        return data_copy\n    \n    def missing_data_simulation(self, \n                               data: Union[pd.Series, pd.DataFrame],\n                               missing_probability: float = 0.1,\n                               missing_pattern: str = 'random',\n                               preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Simulate missing data patterns in economic time series.\"\"\"\n        data_copy = data.copy()\n        n = len(data)\n        \n        if missing_pattern == 'random':\n            # Random missing values\n            missing_mask = np.random.random(n) < missing_probability\n        elif missing_pattern == 'block':\n            # Missing blocks\n            block_size = max(1, int(n * missing_probability / 3))  # 3 blocks\n            missing_mask = np.zeros(n, dtype=bool)\n            for _ in range(3):\n                start_idx = np.random.randint(0, n - block_size + 1)\n                missing_mask[start_idx:start_idx + block_size] = True\n        else:\n            # Default to random\n            missing_mask = np.random.random(n) < missing_probability\n        \n        # Apply missing values\n        if isinstance(data, pd.Series):\n            data_copy[missing_mask] = np.nan\n        else:\n            for col in data.columns:\n                col_missing_mask = np.random.random(n) < missing_probability\n                data_copy.loc[col_missing_mask, col] = np.nan\n        \n        return data_copy\n    \n    def regime_switching(self, \n                        data: Union[pd.Series, pd.DataFrame],\n                        n_regimes: int = 2,\n                        switch_probability: float = 0.1,\n                        preserve_properties: bool = True) -> Union[pd.Series, pd.DataFrame]:\n        \"\"\"Simulate regime switching in economic data.\"\"\"\n        if isinstance(data, pd.Series):\n            return self._regime_switch_series(data, n_regimes, switch_probability)\n        else:\n            regime_data = data.copy()\n            for col in data.columns:\n                regime_data[col] = self._regime_switch_series(\n                    data[col], n_regimes, switch_probability\n                )\n            return regime_data\n    \n    def _regime_switch_series(self, series: pd.Series, \n                             n_regimes: int, \n                             switch_probability: float) -> pd.Series:\n        \"\"\"Apply regime switching to a single series.\"\"\"\n        n = len(series)\n        data_copy = series.copy()\n        \n        # Define regime characteristics\n        regime_multipliers = np.random.uniform(0.5, 2.0, n_regimes)\n        current_regime = 0\n        \n        for i in range(n):\n            # Check for regime switch\n            if np.random.random() < switch_probability:\n                current_regime = np.random.randint(0, n_regimes)\n            \n            # Apply regime effect\n            regime_effect = (regime_multipliers[current_regime] - 1) * 0.1  # 10% max effect\n            data_copy.iloc[i] += regime_effect * abs(data_copy.iloc[i])\n        \n        # Apply constraints\n        if series.name in self.economic_constraints:\n            constraints = self.economic_constraints[series.name]\n            data_copy = np.clip(data_copy, constraints['min'], constraints['max'])\n        \n        return data_copy\n    \n    def augment_for_rare_events(self, \n                               data: Union[pd.Series, pd.DataFrame],\n                               event_types: List[str] = None,\n                               event_intensity: float = 2.0) -> List[Union[pd.Series, pd.DataFrame]]:\n        \"\"\"Generate augmented data specifically for rare economic events.\"\"\"\n        if event_types is None:\n            event_types = ['financial_crisis', 'recession', 'inflation_spike', 'market_crash']\n        \n        augmented_samples = []\n        \n        for event_type in event_types:\n            if event_type == 'financial_crisis':\n                # High volatility, negative trend\n                sample = self.noise_injection(data, noise_level=0.2)\n                sample = self.trend_modification(sample, trend_strength=-0.005)\n                sample = self.outlier_injection(sample, outlier_probability=0.15, \n                                              outlier_magnitude=event_intensity)\n                \n            elif event_type == 'recession':\n                # Sustained negative trend\n                sample = self.trend_modification(data, trend_strength=-0.008)\n                sample = self.seasonal_adjustment(sample, seasonal_strength=0.5)\n                \n            elif event_type == 'inflation_spike':\n                # Sharp increase with high volatility\n                sample = self.trend_modification(data, trend_strength=0.01)\n                sample = self.noise_injection(sample, noise_level=0.15)\n                \n            elif event_type == 'market_crash':\n                # Sudden large drops\n                sample = self.outlier_injection(data, outlier_probability=0.1, \n                                              outlier_magnitude=-event_intensity)\n                sample = self.regime_switching(sample, n_regimes=3, switch_probability=0.2)\n            else:\n                # Default: high volatility\n                sample = self.noise_injection(data, noise_level=0.3)\n            \n            augmented_samples.append(sample)\n        \n        logger.info(f\"Generated {len(augmented_samples)} rare event samples\")\n        return augmented_samples\n    \n    def validate_augmentation_quality(self, \n                                    original_data: Union[pd.Series, pd.DataFrame],\n                                    augmented_data: List[Union[pd.Series, pd.DataFrame]]) -> Dict[str, float]:\n        \"\"\"Validate the quality of augmented data.\"\"\"\n        if isinstance(original_data, pd.Series):\n            return self._validate_series_quality(original_data, augmented_data)\n        else:\n            # Validate each column\n            quality_metrics = {}\n            for col in original_data.columns:\n                col_augmented = [sample[col] for sample in augmented_data if col in sample.columns]\n                col_metrics = self._validate_series_quality(original_data[col], col_augmented)\n                for metric, value in col_metrics.items():\n                    quality_metrics[f'{col}_{metric}'] = value\n            \n            return quality_metrics\n    \n    def _validate_series_quality(self, \n                                original: pd.Series, \n                                augmented_list: List[pd.Series]) -> Dict[str, float]:\n        \"\"\"Validate quality for a single series.\"\"\"\n        if not augmented_list:\n            return {}\n        \n        # Statistical similarity\n        orig_mean = original.mean()\n        orig_std = original.std()\n        orig_skew = stats.skew(original.dropna())\n        orig_kurt = stats.kurtosis(original.dropna())\n        \n        # Calculate metrics for augmented data\n        aug_means = [sample.mean() for sample in augmented_list]\n        aug_stds = [sample.std() for sample in augmented_list]\n        aug_skews = [stats.skew(sample.dropna()) for sample in augmented_list]\n        aug_kurts = [stats.kurtosis(sample.dropna()) for sample in augmented_list]\n        \n        quality_metrics = {\n            'mean_preservation': 1 - abs(np.mean(aug_means) - orig_mean) / abs(orig_mean + 1e-8),\n            'std_preservation': 1 - abs(np.mean(aug_stds) - orig_std) / abs(orig_std + 1e-8),\n            'skewness_preservation': 1 - abs(np.mean(aug_skews) - orig_skew) / (abs(orig_skew) + 1),\n            'kurtosis_preservation': 1 - abs(np.mean(aug_kurts) - orig_kurt) / (abs(orig_kurt) + 1),\n            'diversity_score': np.std(aug_means) / (orig_std + 1e-8),  # Higher = more diverse\n            'sample_count': len(augmented_list)\n        }\n        \n        return quality_metrics\n    \n    def export_augmented_dataset(self, \n                                augmented_data: List[Union[pd.Series, pd.DataFrame]],\n                                filename: str,\n                                format: str = 'csv') -> str:\n        \"\"\"Export augmented dataset to file.\"\"\"\n        if format == 'csv':\n            if isinstance(augmented_data[0], pd.Series):\n                # Convert list of series to DataFrame\n                combined_df = pd.DataFrame({\n                    f'sample_{i}': sample.values \n                    for i, sample in enumerate(augmented_data)\n                })\n                combined_df.to_csv(filename, index=False)\n            else:\n                # Concatenate DataFrames with sample identifier\n                combined_data = []\n                for i, sample in enumerate(augmented_data):\n                    sample_copy = sample.copy()\n                    sample_copy['sample_id'] = i\n                    combined_data.append(sample_copy)\n                \n                combined_df = pd.concat(combined_data, ignore_index=True)\n                combined_df.to_csv(filename, index=False)\n        \n        elif format == 'hdf5':\n            import h5py\n            with h5py.File(filename, 'w') as f:\n                for i, sample in enumerate(augmented_data):\n                    if isinstance(sample, pd.Series):\n                        f.create_dataset(f'sample_{i}', data=sample.values)\n                    else:\n                        grp = f.create_group(f'sample_{i}')\n                        for col in sample.columns:\n                            grp.create_dataset(col, data=sample[col].values)\n        \n        logger.info(f\"Exported {len(augmented_data)} augmented samples to {filename}\")\n        return filename\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    augmentor = EconomicDataAugmentor()\n    \n    # Create sample economic data\n    np.random.seed(42)\n    dates = pd.date_range('2020-01-01', '2023-12-01', freq='M')\n    \n    # GDP growth with trend and seasonality\n    trend = np.linspace(0.02, 0.025, len(dates))\n    seasonal = 0.005 * np.sin(2 * np.pi * np.arange(len(dates)) / 12)\n    noise = np.random.normal(0, 0.003, len(dates))\n    gdp_growth = trend + seasonal + noise\n    \n    economic_series = pd.Series(gdp_growth, index=dates, name='gdp_growth')\n    \n    print(\"Original data statistics:\")\n    print(f\"  Mean: {economic_series.mean():.4f}\")\n    print(f\"  Std: {economic_series.std():.4f}\")\n    print(f\"  Length: {len(economic_series)}\")\n    \n    # Apply various augmentation techniques\n    print(\"\\nApplying augmentation techniques...\")\n    \n    # Noise injection\n    noisy_data = augmentor.noise_injection(economic_series, noise_level=0.1)\n    print(f\"Noise injection: mean={noisy_data.mean():.4f}, std={noisy_data.std():.4f}\")\n    \n    # Time warping\n    warped_data = augmentor.time_warping(economic_series, warp_factor=0.2)\n    print(f\"Time warping: mean={warped_data.mean():.4f}, std={warped_data.std():.4f}\")\n    \n    # Magnitude scaling\n    scaled_data = augmentor.magnitude_scaling(economic_series, scale_range=(0.9, 1.1))\n    print(f\"Magnitude scaling: mean={scaled_data.mean():.4f}, std={scaled_data.std():.4f}\")\n    \n    # Outlier injection\n    outlier_data = augmentor.outlier_injection(economic_series, outlier_probability=0.05)\n    print(f\"Outlier injection: mean={outlier_data.mean():.4f}, std={outlier_data.std():.4f}\")\n    \n    # Full dataset augmentation\n    augmented_dataset = augmentor.augment_dataset(\n        economic_series, \n        techniques=['noise_injection', 'time_warping', 'magnitude_scaling'],\n        augmentation_factor=3\n    )\n    \n    print(f\"\\nAugmented dataset: {len(augmented_dataset)} samples\")\n    \n    # Generate rare event samples\n    rare_event_samples = augmentor.augment_for_rare_events(\n        economic_series,\n        event_types=['financial_crisis', 'recession'],\n        event_intensity=2.5\n    )\n    \n    print(f\"Rare event samples: {len(rare_event_samples)}\")\n    \n    # Validate augmentation quality\n    quality_metrics = augmentor.validate_augmentation_quality(\n        economic_series, \n        augmented_dataset[1:]  # Exclude original\n    )\n    \n    print(\"\\nAugmentation quality metrics:\")\n    for metric, value in quality_metrics.items():\n        print(f\"  {metric}: {value:.3f}\")\n    \n    # Export sample\n    export_file = augmentor.export_augmented_dataset(\n        augmented_dataset[:5],  # First 5 samples\n        'sample_augmented_data.csv'\n    )\n    print(f\"\\nExported sample data to: {export_file}\")\n    \n    print(\"\\nEconomic data augmentation completed\")"