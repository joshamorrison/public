"""
Model Retraining Schedule
Weekly automated model retraining and performance monitoring
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.amazon.aws.operators.sagemaker import SageMakerCreateModelOperator
from airflow.sensors.filesystem import FileSensor
import os
import sys
import pandas as pd
import numpy as np

# Add project root to path
sys.path.append('/opt/airflow/dags/media-mix-modeling')

# Import MMM modules
from models.mmm.econometric_mmm import EconometricMMM
from models.mmm.attribution_models import AttributionModeler
from src.mlflow_integration import setup_mlflow_tracking

# Default arguments for the DAG
default_args = {
    'owner': 'ml-engineering',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=15),
    'catchup': False
}

# Define the DAG
dag = DAG(
    'model_retraining_schedule',
    default_args=default_args,
    description='Weekly MMM model retraining and performance monitoring',
    schedule_interval='0 2 * * 0',  # Weekly on Sunday at 2 AM
    max_active_runs=1,
    tags=['ml', 'retraining', 'weekly', 'production']
)

def evaluate_model_performance(**context):
    """Evaluate current model performance and determine if retraining is needed"""
    print("[EVALUATION] Evaluating current model performance...")
    
    # Initialize MLflow tracker for evaluation
    mlflow_tracker = setup_mlflow_tracking("model-performance-evaluation")
    run_id = mlflow_tracker.start_mmm_run(
        run_name=f"performance_eval_{context['ds']}",
        tags={
            "evaluation_type": "weekly_performance_check",
            "evaluation_date": context['ds']
        }
    )
    
    # Simulate model performance evaluation
    # In production, this would load the latest production model and evaluate on recent data
    
    current_performance = {
        'model_version': 'mmm_v2.1.3',
        'r2_score': np.random.normal(0.75, 0.05),  # Simulate some variance
        'mape': np.random.normal(0.08, 0.02),
        'drift_score': np.random.normal(0.15, 0.05),
        'data_freshness_days': 1,
        'last_training_date': '2025-01-10'
    }
    
    # Performance thresholds
    performance_thresholds = {
        'min_r2': 0.65,
        'max_mape': 0.15,
        'max_drift': 0.25,
        'max_data_age_days': 7
    }
    
    # Determine if retraining is needed
    retraining_triggers = []
    
    if current_performance['r2_score'] < performance_thresholds['min_r2']:
        retraining_triggers.append(f"RÂ² below threshold: {current_performance['r2_score']:.3f} < {performance_thresholds['min_r2']}")
    
    if current_performance['mape'] > performance_thresholds['max_mape']:
        retraining_triggers.append(f"MAPE above threshold: {current_performance['mape']:.3f} > {performance_thresholds['max_mape']}")
    
    if current_performance['drift_score'] > performance_thresholds['max_drift']:
        retraining_triggers.append(f"Data drift detected: {current_performance['drift_score']:.3f} > {performance_thresholds['max_drift']}")
    
    if current_performance['data_freshness_days'] > performance_thresholds['max_data_age_days']:
        retraining_triggers.append(f"Data staleness: {current_performance['data_freshness_days']} days > {performance_thresholds['max_data_age_days']}")
    
    evaluation_results = {
        'evaluation_date': context['ds'],
        'current_performance': current_performance,
        'performance_thresholds': performance_thresholds,
        'retraining_needed': len(retraining_triggers) > 0,
        'retraining_triggers': retraining_triggers,
        'evaluation_run_id': run_id
    }
    
    # Log to MLflow
    mlflow_tracker.log_mmm_performance(current_performance)
    mlflow_tracker.log_artifacts_json(evaluation_results, "performance_evaluation.json")
    mlflow_tracker.end_run()
    
    context['task_instance'].xcom_push(key='evaluation_results', value=evaluation_results)
    
    print(f"[EVALUATION] Performance evaluation completed")
    print(f"[EVALUATION] Retraining needed: {evaluation_results['retraining_needed']}")
    if retraining_triggers:
        for trigger in retraining_triggers:
            print(f"[TRIGGER] {trigger}")
    
    return "Model performance evaluation completed"

def prepare_training_data(**context):
    """Prepare and validate training data for model retraining"""
    print("[DATA-PREP] Preparing training data for model retraining...")
    
    evaluation_results = context['task_instance'].xcom_pull(key='evaluation_results')
    
    if not evaluation_results.get('retraining_needed', False):
        print("[DATA-PREP] Retraining not needed, skipping data preparation")
        return "Data preparation skipped - retraining not needed"
    
    # Simulate data preparation process
    # In production, this would fetch the latest data from data warehouse
    
    training_data_summary = {
        'preparation_date': context['ds'],
        'total_records': np.random.randint(5000, 10000),
        'date_range_start': '2023-01-01',
        'date_range_end': context['ds'],
        'channels_included': ['tv', 'digital', 'radio', 'print', 'social'],
        'data_quality_score': np.random.uniform(0.85, 0.98),
        'missing_data_pct': np.random.uniform(0.01, 0.05),
        'outliers_detected': np.random.randint(5, 25)
    }
    
    # Data validation checks
    validation_issues = []
    
    if training_data_summary['data_quality_score'] < 0.8:
        validation_issues.append("Data quality score below threshold")
    
    if training_data_summary['missing_data_pct'] > 0.1:
        validation_issues.append("High percentage of missing data")
    
    if training_data_summary['outliers_detected'] > 50:
        validation_issues.append("Excessive outliers detected")
    
    training_data_summary['validation_passed'] = len(validation_issues) == 0
    training_data_summary['validation_issues'] = validation_issues
    
    context['task_instance'].xcom_push(key='training_data', value=training_data_summary)
    
    print(f"[DATA-PREP] Prepared {training_data_summary['total_records']} records for training")
    print(f"[DATA-PREP] Data validation passed: {training_data_summary['validation_passed']}")
    
    return "Training data preparation completed"

def retrain_mmm_models(**context):
    """Retrain MMM models with latest data"""
    print("[RETRAINING] Starting MMM model retraining...")
    
    evaluation_results = context['task_instance'].xcom_pull(key='evaluation_results')
    training_data = context['task_instance'].xcom_pull(key='training_data')
    
    if not evaluation_results.get('retraining_needed', False):
        print("[RETRAINING] Retraining not needed, skipping model training")
        return "Model retraining skipped - not needed"
    
    if not training_data.get('validation_passed', False):
        print("[RETRAINING] Data validation failed, aborting retraining")
        return "Model retraining aborted - data validation failed"
    
    # Initialize MLflow tracking for retraining
    mlflow_tracker = setup_mlflow_tracking("model-retraining")
    run_id = mlflow_tracker.start_mmm_run(
        run_name=f"retraining_{context['ds']}",
        tags={
            "retraining_type": "weekly_scheduled",
            "retraining_date": context['ds'],
            "previous_model": evaluation_results['current_performance']['model_version'],
            "trigger_reasons": ", ".join(evaluation_results['retraining_triggers'])
        }
    )
    
    # Simulate model retraining with multiple configurations
    model_configurations = [
        {'adstock_rate': 0.4, 'saturation_param': 0.5, 'regularization_alpha': 0.05},
        {'adstock_rate': 0.5, 'saturation_param': 0.6, 'regularization_alpha': 0.1},
        {'adstock_rate': 0.6, 'saturation_param': 0.7, 'regularization_alpha': 0.15}
    ]
    
    best_model_results = None
    best_performance = 0
    
    for i, config in enumerate(model_configurations):
        print(f"[RETRAINING] Training model configuration {i+1}/3...")
        
        # Simulate model training
        model_results = {
            'configuration': config,
            'r2_score': np.random.normal(0.78, 0.03),
            'mape': np.random.normal(0.07, 0.01),
            'training_time_minutes': np.random.uniform(15, 45),
            'convergence': np.random.choice([True, True, True, False], p=[0.7, 0.2, 0.08, 0.02])
        }
        
        # Log configuration to MLflow
        mlflow_tracker.log_mmm_model(
            model=None,  # Would be actual model in production
            model_type=f"econometric_mmm_config_{i+1}",
            parameters=config
        )
        
        mlflow_tracker.log_mmm_performance({
            'r2': model_results['r2_score'],
            'mape': model_results['mape']
        })
        
        # Select best model
        if model_results['r2_score'] > best_performance and model_results['convergence']:
            best_performance = model_results['r2_score']
            best_model_results = model_results
    
    # Training summary
    retraining_results = {
        'retraining_date': context['ds'],
        'configurations_tested': len(model_configurations),
        'best_model': best_model_results,
        'performance_improvement': best_performance - evaluation_results['current_performance']['r2_score'],
        'new_model_version': f"mmm_v2.2.{context['ds'].replace('-', '')}",
        'mlflow_run_id': run_id,
        'ready_for_deployment': best_model_results is not None and best_performance > 0.7
    }
    
    # Log final results
    mlflow_tracker.log_artifacts_json(retraining_results, "retraining_results.json")
    mlflow_tracker.end_run()
    
    context['task_instance'].xcom_push(key='retraining_results', value=retraining_results)
    
    print(f"[RETRAINING] Model retraining completed")
    print(f"[RETRAINING] Best RÂ²: {best_performance:.3f}")
    print(f"[RETRAINING] Performance improvement: {retraining_results['performance_improvement']:+.3f}")
    
    return "MMM model retraining completed"

def validate_new_models(**context):
    """Validate newly trained models before deployment"""
    print("[VALIDATION] Validating newly trained models...")
    
    retraining_results = context['task_instance'].xcom_pull(key='retraining_results')
    
    if not retraining_results:
        print("[VALIDATION] No retraining results found, skipping validation")
        return "Model validation skipped - no new models"
    
    # Model validation tests
    validation_tests = {
        'performance_test': retraining_results['best_model']['r2_score'] > 0.65,
        'convergence_test': retraining_results['best_model']['convergence'],
        'improvement_test': retraining_results['performance_improvement'] > -0.05,  # Allow small degradation
        'stability_test': retraining_results['best_model']['mape'] < 0.15,
        'bias_test': True,  # Would run actual bias tests in production
        'fairness_test': True  # Would run fairness tests in production
    }
    
    validation_summary = {
        'validation_date': context['ds'],
        'model_version': retraining_results['new_model_version'],
        'tests_run': len(validation_tests),
        'tests_passed': sum(validation_tests.values()),
        'validation_passed': all(validation_tests.values()),
        'test_details': validation_tests
    }
    
    context['task_instance'].xcom_push(key='validation_summary', value=validation_summary)
    
    print(f"[VALIDATION] Model validation completed")
    print(f"[VALIDATION] Tests passed: {validation_summary['tests_passed']}/{validation_summary['tests_run']}")
    print(f"[VALIDATION] Overall validation: {'PASSED' if validation_summary['validation_passed'] else 'FAILED'}")
    
    return "Model validation completed"

def deploy_models(**context):
    """Deploy validated models to production"""
    print("[DEPLOYMENT] Deploying models to production...")
    
    validation_summary = context['task_instance'].xcom_pull(key='validation_summary')
    retraining_results = context['task_instance'].xcom_pull(key='retraining_results')
    
    if not validation_summary or not validation_summary.get('validation_passed', False):
        print("[DEPLOYMENT] Model validation failed, aborting deployment")
        return "Model deployment aborted - validation failed"
    
    # Deployment simulation
    deployment_results = {
        'deployment_date': context['ds'],
        'model_version': retraining_results['new_model_version'],
        'deployment_environment': 'production',
        'rollback_version': 'mmm_v2.1.3',
        'deployment_strategy': 'blue_green',
        'health_checks_passed': True,
        'deployment_successful': True
    }
    
    # In production, this would:
    # 1. Deploy to staging environment
    # 2. Run integration tests
    # 3. Deploy to production with blue-green strategy
    # 4. Monitor initial performance
    # 5. Complete deployment or rollback
    
    context['task_instance'].xcom_push(key='deployment_results', value=deployment_results)
    
    print(f"[DEPLOYMENT] Model deployment completed")
    print(f"[DEPLOYMENT] New model version: {deployment_results['model_version']}")
    print(f"[DEPLOYMENT] Deployment successful: {deployment_results['deployment_successful']}")
    
    return "Model deployment completed"

def monitor_deployment(**context):
    """Monitor deployed model performance"""
    print("[MONITORING] Monitoring deployed model performance...")
    
    deployment_results = context['task_instance'].xcom_pull(key='deployment_results')
    
    if not deployment_results or not deployment_results.get('deployment_successful', False):
        print("[MONITORING] No successful deployment to monitor")
        return "Deployment monitoring skipped - no successful deployment"
    
    # Simulate post-deployment monitoring
    monitoring_results = {
        'monitoring_date': context['ds'],
        'model_version': deployment_results['model_version'],
        'monitoring_duration_hours': 24,
        'performance_stable': True,
        'error_rate': np.random.uniform(0.001, 0.01),
        'prediction_latency_ms': np.random.uniform(50, 150),
        'throughput_requests_per_second': np.random.uniform(10, 50),
        'alerts_triggered': 0
    }
    
    # Performance checks
    if monitoring_results['error_rate'] > 0.05:
        monitoring_results['alerts_triggered'] += 1
        print("[ALERT] High error rate detected")
    
    if monitoring_results['prediction_latency_ms'] > 500:
        monitoring_results['alerts_triggered'] += 1
        print("[ALERT] High prediction latency detected")
    
    context['task_instance'].xcom_push(key='monitoring_results', value=monitoring_results)
    
    print(f"[MONITORING] Deployment monitoring completed")
    print(f"[MONITORING] Performance stable: {monitoring_results['performance_stable']}")
    print(f"[MONITORING] Alerts triggered: {monitoring_results['alerts_triggered']}")
    
    return "Deployment monitoring completed"

def generate_retraining_report(**context):
    """Generate comprehensive retraining report"""
    print("[REPORTING] Generating retraining report...")
    
    # Collect all results
    evaluation_results = context['task_instance'].xcom_pull(key='evaluation_results')
    retraining_results = context['task_instance'].xcom_pull(key='retraining_results')
    validation_summary = context['task_instance'].xcom_pull(key='validation_summary')
    deployment_results = context['task_instance'].xcom_pull(key='deployment_results')
    monitoring_results = context['task_instance'].xcom_pull(key='monitoring_results')
    
    # Compile comprehensive report
    retraining_report = {
        'report_date': context['ds'],
        'retraining_cycle': 'weekly',
        'pipeline_status': 'SUCCESS',
        'evaluation_summary': evaluation_results,
        'retraining_summary': retraining_results,
        'validation_summary': validation_summary,
        'deployment_summary': deployment_results,
        'monitoring_summary': monitoring_results,
        'key_metrics': {
            'performance_improvement': retraining_results.get('performance_improvement', 0) if retraining_results else 0,
            'deployment_successful': deployment_results.get('deployment_successful', False) if deployment_results else False,
            'validation_passed': validation_summary.get('validation_passed', False) if validation_summary else False
        },
        'recommendations': []
    }
    
    # Add recommendations
    if retraining_report['key_metrics']['performance_improvement'] < 0:
        retraining_report['recommendations'].append("Monitor model performance closely - slight degradation observed")
    
    if retraining_report['key_metrics']['performance_improvement'] > 0.1:
        retraining_report['recommendations'].append("Significant improvement achieved - consider updating training frequency")
    
    context['task_instance'].xcom_push(key='retraining_report', value=retraining_report)
    
    print(f"[REPORTING] Retraining report generated")
    print(f"[REPORTING] Pipeline status: {retraining_report['pipeline_status']}")
    print(f"[REPORTING] Recommendations: {len(retraining_report['recommendations'])}")
    
    return "Retraining report generated"

# Define tasks
evaluate_performance_task = PythonOperator(
    task_id='evaluate_model_performance',
    python_callable=evaluate_model_performance,
    dag=dag
)

prepare_data_task = PythonOperator(
    task_id='prepare_training_data',
    python_callable=prepare_training_data,
    dag=dag
)

retrain_models_task = PythonOperator(
    task_id='retrain_mmm_models',
    python_callable=retrain_mmm_models,
    dag=dag
)

validate_models_task = PythonOperator(
    task_id='validate_new_models',
    python_callable=validate_new_models,
    dag=dag
)

deploy_models_task = PythonOperator(
    task_id='deploy_models',
    python_callable=deploy_models,
    dag=dag
)

monitor_deployment_task = PythonOperator(
    task_id='monitor_deployment',
    python_callable=monitor_deployment,
    dag=dag
)

generate_report_task = PythonOperator(
    task_id='generate_retraining_report',
    python_callable=generate_retraining_report,
    dag=dag
)

# Set task dependencies
evaluate_performance_task >> prepare_data_task >> retrain_models_task >> validate_models_task >> deploy_models_task >> monitor_deployment_task >> generate_report_task