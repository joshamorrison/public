# Airflow Configuration for Media Mix Modeling Platform
# Production-ready configuration with security and monitoring

[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live
dags_folder = /opt/airflow/dags

# The folder where airflow plugins are stored
plugins_folder = /opt/airflow/plugins

# Base log folder for airflow
base_log_folder = /opt/airflow/logs

# Logging level
logging_level = INFO

# Executor to use
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# Load examples
load_examples = False

# Default timezone
default_timezone = UTC

# Maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Number of seconds to retain log files for the LocalExecutor
log_retention_days = 30

[scheduler]
# Job heartbeat interval in seconds
job_heartbeat_sec = 5

# The number of seconds to wait between consecutive scheduler runs
min_file_process_interval = 30

# How often should stats be printed to the logs
print_stats_interval = 30

# Maximum number of threads to use to handle individual DAG files
max_threads = 2

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
max_active_tasks_per_dag = 16

# How long before timing out a DAG file processor
dag_dir_list_interval = 300

# How often to scan the DAGs directory for new files
dag_processor_timeout = 60

[webserver]
# The base URL of your website as airflow cannot guess what domain or cname you are using
base_url = http://localhost:8080

# Default port for Airflow webserver
web_server_port = 8080

# Secret key for securing cookies
secret_key = mmm_platform_secret_key_2025

# Enable authentication
authenticate = True

# Authentication backend
auth_backend = airflow.contrib.auth.backends.password_auth

# Workers to run the Gunicorn web server
workers = 4

# Worker timeout
worker_timeout = 120

# DAG default view
dag_default_view = tree

# Page size for DAGs
dag_page_size = 25

# Page size for task instances
page_size = 100

# Enable auto-refresh of webserver
auto_refresh_interval = 30

[email]
# Email backend
email_backend = airflow.providers.smtp.hooks.smtp.SmtpHook

[smtp]
# SMTP configuration for notifications
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@mmm-platform.com

[logging]
# The folder where airflow logs are stored
base_log_folder = /opt/airflow/logs

# The executor class that airflow should use
remote_logging = False

# Logging level
logging_level = INFO

# Log filename template
log_filename_template = dag_id={{ dag_id }}/run_id={{ run_id }}/task_id={{ task_id }}/attempt={{ try_number }}.log

[metrics]
# Enable StatsD metrics
statsd_on = True
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow.mmm

[celery]
# Celery configuration for distributed execution
broker_url = redis://redis:6379/0
result_backend = db+postgresql://airflow:airflow@postgres:5432/airflow

# Celery worker concurrency
worker_concurrency = 16

[operators]
# Default owner for new DAGs
default_owner = mmm-team

# Default retries for operators
default_retries = 2

[secrets]
# Backend to use for storing secrets
backend = airflow.providers.hashicorp.secrets.vault.VaultBackend
backend_kwargs = {"connections_path": "connections", "variables_path": "variables", "mount_point": "airflow", "url": "http://vault:8200"}

[api]
# Enable the experimental API
auth_backend = airflow.api.auth.backend.basic_auth

[lineage]
# Enable lineage tracking
backend = 

[atlas]
# Atlas configuration for metadata management
sasl_enabled = False
host = localhost
port = 21000
username = admin
password = admin

[kubernetes]
# Kubernetes configuration for KubernetesPodOperator
namespace = airflow
airflow_configmap = airflow-config
worker_container_repository = apache/airflow
worker_container_tag = 2.7.0
worker_container_image_pull_policy = IfNotPresent
delete_worker_pods = True

[kubernetes_secrets]
# Kubernetes secrets configuration
AIRFLOW__CORE__SQL_ALCHEMY_CONN = airflow-secrets=sql_alchemy_conn
AIRFLOW__CORE__FERNET_KEY = airflow-secrets=fernet_key

[mlflow]
# MLflow integration configuration
tracking_uri = http://mlflow:5000
experiment_name = media-mix-modeling
artifact_location = s3://mmm-artifacts

[mmm_platform]
# Custom MMM platform configuration
data_sources = kaggle,huggingface,google_ads,facebook_ads
model_types = econometric_mmm,attribution_models
optimization_methods = multi_objective,gradient_descent
reporting_formats = json,csv,pdf
notification_channels = email,slack
deployment_environments = staging,production